<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Credit Card Fraud Detection | vipul </title> <meta name="author" content="Vipul Yadav"> <meta name="description" content="A simple website to showcase my projects. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="data, machine learning, data analytics, data scientist, jekyll, jekyll-theme, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo2.svg?832f2faeb56294274c8f6407f2dfa468"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="vipuly4dav.github.io/projects/project_credit_card_fraud_detection/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style>html{font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Credit Card Fraud Detection",
            "description": "",
            "published": "March 08, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$$",
                  "right": "$$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
              }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="/"> <svg> <image xlink:href="/assets/img/logo2.svg?832f2faeb56294274c8f6407f2dfa468"></image> </svg> </a> </div> <a class="navbar-brand title font-weight-lighter" href="/"> vipul </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Credit Card Fraud Detection</h1> <p></p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h1"> <a href="#methodology">Methodology</a> <ul> <li class="toc-entry toc-h2"><a href="#libraries-used">Libraries used</a></li> <li class="toc-entry toc-h2"> <a href="#defining-the-problem">Defining the problem</a> <ul> <li class="toc-entry toc-h3"><a href="#similar-problems">Similar problems</a></li> <li class="toc-entry toc-h3"><a href="#current-solutions">Current solutions</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#exploring-the-data">Exploring the data</a> <ul> <li class="toc-entry toc-h3"><a href="#assumptions-about-the-data">Assumptions about the data</a></li> <li class="toc-entry toc-h3"><a href="#plots">Plots</a></li> <li class="toc-entry toc-h3"><a href="#mann-whitney-u-test">Mann-Whitney U test</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#modelling">Modelling</a> <ul> <li class="toc-entry toc-h3"><a href="#evaluation-metric">Evaluation metric</a></li> <li class="toc-entry toc-h3"><a href="#test-harness">Test harness</a></li> <li class="toc-entry toc-h3"><a href="#balancing">Balancing</a></li> <li class="toc-entry toc-h3"><a href="#spot-testing-various-models">Spot testing various models</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#hyperparameter-tuning">Hyperparameter tuning</a> <ul> <li class="toc-entry toc-h3"><a href="#histogram-gradient-boosting">Histogram Gradient Boosting</a></li> <li class="toc-entry toc-h3"><a href="#random-forest">Random Forest</a></li> <li class="toc-entry toc-h2"><a href="#results">Results</a></li> </ul> </li> <li class="toc-entry toc-h1"><a href="#conclusion">Conclusion</a></li> <li class="toc-entry toc-h1"><a href="#future-work">Future work</a></li> </ul> </nav> </d-contents> <p>See the jupyter notebook <a href="/assets/html/credit_card_fraud_detection.html"> here </a></p> <h1 id="introduction"> <a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h1> <p>With the growth in spread of digital payment technology, cases of digital fraud are also on the rise. It is imperative that systems have and will be evolved to tackle the problem of fraudulent transactions. In this vein, this project takes up a credit card dataset from <a href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud" rel="external nofollow noopener" target="_blank">Kaggle</a> and tries to evolve a machine learning program that can identify fraudulent transactions. Informally speaking, a program is needed that will find transactions which are fraudulent, depending on various data points associated with a given transaction, before the transaction has completed.</p> <p>In the given dataset, recorded over a two day period, transactions worth € 60,128 are marked as fraudulent. This constitutes 0.24% of the total value of the transaction. Although the value in percentage terms seems small due to the large scale nature of operation involved, in absolute terms, the losses are immense. If the predictions from the model reduce the number of fraudulent transactions, it will result in huge savings in the long run.</p> <hr> <h1 id="methodology"> <a class="anchor" href="#methodology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Methodology</h1> <h2 id="libraries-used"> <a class="anchor" href="#libraries-used" aria-hidden="true"><span class="octicon octicon-link"></span></a>Libraries used</h2> <ol> <li>For handling data : Pandas, Numpy</li> <li>For visualization : Matplotlib, Seaborn</li> <li>For machine learning : Scikit-learn, imbalanced-learn, SciPy</li> </ol> <h2 id="defining-the-problem"> <a class="anchor" href="#defining-the-problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Defining the problem</h2> <p>In order to give our problem a formal footing, we can look at the following definition of machine learning as given by Tom Mitchell :</p> <blockquote> <p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p> </blockquote> <p>Conforming our problem to this definition, we can define our unknowns as :</p> <ul> <li>Task (T) : To classify a transaction that hasn’t yet been fully processed to be fraudulent or non-fraudulent</li> <li>Experience (E) : A collection of transactions where some are fraudulent and some are not</li> <li>Performance (P) : The number of transactions which are non fraudulent</li> </ul> <h3 id="similar-problems"> <a class="anchor" href="#similar-problems" aria-hidden="true"><span class="octicon octicon-link"></span></a>Similar problems</h3> <p>Due to the unique nature of this problem, where instances of interest occur with a rare frequency, this problem is similar to anomaly detection and outlier detection.</p> <h3 id="current-solutions"> <a class="anchor" href="#current-solutions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Current solutions</h3> <p>The current solution to finding fraudulent transactions are enumerated as follows:</p> <ul> <li>Rule based systems for e.g. if transaction exceeds a spending limit or if IP address differs from shipping address</li> <li>Multi factor authentication</li> <li>One time passwords</li> <li>3D Secure (3DS)</li> <li>Address Verification Service (AVS)</li> <li>Card Verification Value (CVV)</li> <li>Biometric verification</li> <li>Data enrichment techniques which augment transaction data with external information, live device data and geo-location.</li> </ul> <h2 id="exploring-the-data"> <a class="anchor" href="#exploring-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Exploring the data</h2> <p>The dataset contains transactions made by European cardholders over a 2 day period in September 2013. The numerical input variables (totaling 28 in number) are arrived at by PCA transformation. The rest of the variables are <code class="language-plaintext highlighter-rouge">Time</code> and <code class="language-plaintext highlighter-rouge">Amount</code>. The variable <code class="language-plaintext highlighter-rouge">Class</code> indicates whether the transaction was fraudulent (1) or non-fraudulent (0).</p> <p>There are no missing values and the total number of instances are around ~285,000. Non-fraudulent transactions consist of an overwhelming majority of 99.83% of instances, whereas fraudulent transactions consist of 0.17%. Over the period of data collection, from the total transaction value of € 25,162,590, € 60,128 was involved in fraudulent transactions, which amounts to 0.24% of total transaction value. Thus, the distribution of <code class="language-plaintext highlighter-rouge">Class</code> is imbalanced.</p> <h3 id="assumptions-about-the-data"> <a class="anchor" href="#assumptions-about-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assumptions about the data</h3> <ol> <li>The data collected over the two day period is representative.</li> <li>The PCA transformed numerical inputs are collected before the transaction is completed.</li> <li>The time at which transaction has occurred does not correlate to <code class="language-plaintext highlighter-rouge">Class</code> </li> </ol> <h3 id="plots"> <a class="anchor" href="#plots" aria-hidden="true"><span class="octicon octicon-link"></span></a>Plots</h3> <p>In order to visualize the distribution of various features, and their distribution when they are grouped by <code class="language-plaintext highlighter-rouge">Class</code>, the following plots are generated. They give a visual intuition regarding the discriminating power of various features. If the feature has the same distribution across the two groups of fraudulent and non-fraudulent transactions then that feature has no discriminating power. From a visual inspection of below graphs, V6, V13, V15, V22, V24, V25, V26 seem to have very similar density distribution across fraudulent and non-fraudulent cases.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/credit_card_fraud_detection/univariate-480.webp 480w,/assets/img/credit_card_fraud_detection/univariate-800.webp 800w,/assets/img/credit_card_fraud_detection/univariate-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/credit_card_fraud_detection/univariate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="univariate plots" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="mann-whitney-u-test"> <a class="anchor" href="#mann-whitney-u-test" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mann-Whitney U test</h3> <p>Mann Whitney U is a non-parametric test to check mean between two groups. We cannot use t-test here because its assumption that the two distributions should be normally distributed is not satisfied. The null hypothesis of Mann-Whitney U test is that the distribution of two populations are identical. On the basis of this test, a few features are barred from further consideration to reduce complexity.</p> <h2 id="modelling"> <a class="anchor" href="#modelling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Modelling</h2> <h3 id="evaluation-metric"> <a class="anchor" href="#evaluation-metric" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluation metric</h3> <p>We need high recall in this situation in order to detect all true fraudulent transactions. However, due to precision-recall tradeoff, a high recall will adversely impact precision. This would translate to poor user experience as many genuine transactions would be classified as fraudulent. So we need an evaluation metric that takes into consideration both precision and recall and that which focuses more on positive (fraudulent) class.</p> <p>F1 score and PR AUC (Precision-Recall Area Under Curve) are two such metrics. F1 will favour precision and recall having similar values, but there is no such consideration required in this scenario. We are vying for a higher recall. Instead of F1 score, we can use F2 score which will give more importance to recall over precision. PR AUC will be a summary metric that will take into account all thresholds and resulting precision and recall.. Hence, we choose two evaluation metrics: Average Precision (discretized version of PR AUC) and F2 score.</p> <h3 id="test-harness"> <a class="anchor" href="#test-harness" aria-hidden="true"><span class="octicon octicon-link"></span></a>Test harness</h3> <p>The dataset is split in an 80:20 ratio where 80% of data will be used for training, and 20% will be kept aside for estimating generalization error at a later stage. Validation error will be calculated by performing k-fold cross validation.</p> <h3 id="balancing"> <a class="anchor" href="#balancing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Balancing</h3> <p>The dataset is imbalanced with respect to the target variable i.e. <code class="language-plaintext highlighter-rouge">Class</code> hence we employ rebalancing techniques. Over-sampling of positive class (fraudulent transactions) is done through SMOTE (Synthetic Minority Oversampling Technique) while under sampling of negative class is done through random under sampling. This provides a more balanced dataset and improves the performance of the model.</p> <h3 id="spot-testing-various-models"> <a class="anchor" href="#spot-testing-various-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Spot testing various models</h3> <p>A variety of algorithms are chosen to spot check with their default configurations. The algorithms range from simple models (Logistics, Linear Discriminant Analysis), to tree based (Decision tree, Random Forest, Gradient Boosted Trees, AdaBoost), and also Support Vector Machines, K-Nearest Neighbors, and Multi Layer Perceptron. Spot check is performed with a smaller, resampled subset of training data in order to improve fitting time. On the basis of this spot check, it is ascertained that the best performing models are tree based ensemble methods (Histogram Gradient Boosting and Random Forest). Histogram Gradient Boosting provides good performance with low train and inference times but both Histogram based Gradient Boosting and Random Forests are overfitting. These are taken up for further hyperparameter tuning.</p> <hr> <h1 id="hyperparameter-tuning"> <a class="anchor" href="#hyperparameter-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hyperparameter tuning</h1> <p>For tuning the models, the size of the training dataset is reduced to 50% through resampling to reduce fitting time. The general methodology employed is to seek an improvement in validation score while also keeping training score in check. Cross validation is performed in order to estimate variance in both. This will ensure that the generalization error is low.</p> <h3 id="histogram-gradient-boosting"> <a class="anchor" href="#histogram-gradient-boosting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Histogram Gradient Boosting</h3> <details><summary>Overview of the process</summary> <ol> <li> <p><code class="language-plaintext highlighter-rouge">max_bins=50</code> gives comparable results to higher values of <code class="language-plaintext highlighter-rouge">max_bin</code>. However, the train score is high so the model is overfitting.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘hgb__max_bins’: 50}</td> <td>0.860728</td> <td>0.049480</td> <td>0.987708</td> <td>0.009457</td> </tr> <tr> <td>{‘hgb__max_bins’: 200}</td> <td>0.858225</td> <td>0.053159</td> <td>0.988027</td> <td>0.010460</td> </tr> <tr> <td>{‘hgb__max_bins’: 255}</td> <td>0.854109</td> <td>0.054755</td> <td>0.979819</td> <td>0.032183</td> </tr> </tbody> </table> </li> <li> <p>When <code class="language-plaintext highlighter-rouge">max_depth=2</code> and <code class="language-plaintext highlighter-rouge">max_bins=50</code> , mean_test_score remains almost the same, but mean_train_score reduces drastically. Setting a low value of <code class="language-plaintext highlighter-rouge">max_depth</code> regularizes the model.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘hgb__max_bins’: 50, ‘hgb__max_depth’: 8}</td> <td>0.860260</td> <td>0.051656</td> <td>0.987766</td> <td>0.009068</td> </tr> <tr> <td>{‘hgb__max_bins’: 50, ‘hgb__max_depth’: 2}</td> <td>0.845519</td> <td>0.048916</td> <td>0.890494</td> <td>0.007762</td> </tr> </tbody> </table> </li> <li> <p>Changing <code class="language-plaintext highlighter-rouge">learning_rate</code> does not affect the results and optimum learning rate is near the default learning rate of 0.1</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 100, ‘hgb__max_depth’: 4}</td> <td>0.854492</td> <td>0.052946</td> <td>0.973363</td> <td>0.017663</td> </tr> <tr> <td>{‘hgb__learning_rate’: 0.2, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 2}</td> <td>0.837929</td> <td>0.041954</td> <td>0.926814</td> <td>0.007448</td> </tr> <tr> <td>{‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 100, ‘hgb__max_depth’: 2}</td> <td>0.834128</td> <td>0.056762</td> <td>0.893284</td> <td>0.009976</td> </tr> </tbody> </table> </li> <li> <p>Another regularization parameter can be <code class="language-plaintext highlighter-rouge">max_features</code> . <code class="language-plaintext highlighter-rouge">max_features = 0.4</code> increases the mean_test_score while also reducing mean_train_score.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 100, ‘hgb__max_depth’: 4, ‘hgb__max_features’: 0.6}</td> <td>0.858904</td> <td>0.054259</td> <td>0.980155</td> <td>0.010521</td> </tr> <tr> <td>{‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 100, ‘hgb__max_depth’: 3, ‘hgb__max_features’: 0.4}</td> <td>0.851308</td> <td>0.053814</td> <td>0.946933</td> <td>0.008449</td> </tr> <tr> <td>{‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 2, ‘hgb__max_features’: 0.4}</td> <td>0.839217</td> <td>0.061345</td> <td>0.866621</td> <td>0.031578</td> </tr> </tbody> </table> </li> <li> <p><code class="language-plaintext highlighter-rouge">min_samples_leaf=3000</code> along with <code class="language-plaintext highlighter-rouge">max_bins=50, max_depth=4, max_features=0.5</code> produce almost the same results. Using <code class="language-plaintext highlighter-rouge">min_samples_leaf</code> in conjunction with other parameters does not provide any benefit either with respect to increase in mean_test_score or reduction in mean_train_score.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘hgb__learning_rate’: 0.2, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 4, ‘hgb__max_features’: 0.5, ‘hgb__min_samples_leaf’: 3000}</td> <td>0.850635</td> <td>0.045820</td> <td>0.953815</td> <td>0.020566</td> </tr> <tr> <td>{‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 4, ‘hgb__max_features’: 0.5, ‘hgb__min_samples_leaf’: 3000}</td> <td>0.829798</td> <td>0.045730</td> <td>0.889496</td> <td>0.035958</td> </tr> </tbody> </table> </li> <li> <p><code class="language-plaintext highlighter-rouge">l2_regularization</code> does not provide any major improvement to the model. Hence, not using this also.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘hgb__l2_regularization’: 0, ‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 4, ‘hgb__max_features’: 0.5}</td> <td>0.854260</td> <td>0.050694</td> <td>0.979205</td> <td>0.007295</td> </tr> <tr> <td>{‘hgb__l2_regularization’: 0, ‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 3, ‘hgb__max_features’: 0.4}</td> <td>0.851472</td> <td>0.061326</td> <td>0.928200</td> <td>0.036915</td> </tr> <tr> <td>{‘hgb__l2_regularization’: 0, ‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 2, ‘hgb__max_features’: 0.4}</td> <td>0.833171</td> <td>0.049059</td> <td>0.868262</td> <td>0.015291</td> </tr> </tbody> </table> </li> <li> <p>Increasing <code class="language-plaintext highlighter-rouge">max_iter</code> provides minimal improvement to the model. Having a high value of <code class="language-plaintext highlighter-rouge">max_iter</code> leads to overfitting.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 4, ‘hgb__max_features’: 0.6, ‘hgb__max_iter’: 1000}</td> <td>0.861113</td> <td>0.050726</td> <td>0.982396</td> <td>0.031535</td> </tr> <tr> <td>{‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 2, ‘hgb__max_features’: 0.6, ‘hgb__max_iter’: 500}</td> <td>0.845474</td> <td>0.063645</td> <td>0.919762</td> <td>0.054989</td> </tr> <tr> <td>{‘hgb__learning_rate’: 0.1, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 2, ‘hgb__max_features’: 0.4, ‘hgb__max_iter’: 100}</td> <td>0.833132</td> <td>0.059743</td> <td>0.876368</td> <td>0.012102</td> </tr> </tbody> </table> </li> <li> <p>Changing <code class="language-plaintext highlighter-rouge">sampling_strategy</code> also provides minimal improvement.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘hgb__learning_rate’: 0.2, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 3, ‘hgb__max_features’: 0.6, ‘rus__sampling_strategy’: 0.05, ‘smote__sampling_strategy’: 0.025}</td> <td>0.856398</td> <td>0.052151</td> <td>0.975792</td> <td>0.005290</td> </tr> <tr> <td>{‘hgb__learning_rate’: 0.2, ‘hgb__max_bins’: 50, ‘hgb__max_depth’: 2, ‘hgb__max_features’: 0.5, ‘rus__sampling_strategy’: 0.05, ‘smote__sampling_strategy’: 0.025}</td> <td>0.838235</td> <td>0.048555</td> <td>0.910981</td> <td>0.014737</td> </tr> </tbody> </table> </li> </ol> </details> <p>The final set of hyperparameters that provide a good fit are :</p> <p><code class="language-plaintext highlighter-rouge">max_bins = 50</code>|<br> <code class="language-plaintext highlighter-rouge">max_depth = 2</code>|<br> <code class="language-plaintext highlighter-rouge">learning_rate = 0.1</code>|<br> <code class="language-plaintext highlighter-rouge">max_features = 0.4</code>|<br> <code class="language-plaintext highlighter-rouge">smote_sampling_strategy =0.025</code>|<br> <code class="language-plaintext highlighter-rouge">randomundersampler_sampling_strategy=0.05</code></p> <h3 id="random-forest"> <a class="anchor" href="#random-forest" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Forest</h3> <details><summary>Overview of the process</summary> <ol> <li> <p>Optimizing for <code class="language-plaintext highlighter-rouge">max_depth</code>, we get the following results. As observed, <code class="language-plaintext highlighter-rouge">max_depth</code> acts as a regularisation parameter, with lower value of <code class="language-plaintext highlighter-rouge">max_depth</code> leading to lower mean_train_score.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘rf__max_depth’: 10}</td> <td>0.837905</td> <td>0.051500</td> <td>0.930738</td> <td>0.009532</td> </tr> <tr> <td>{‘rf__max_depth’: 6}</td> <td>0.826149</td> <td>0.043798</td> <td>0.882363</td> <td>0.007530</td> </tr> </tbody> </table> </li> <li> <p>Upon optimizing <code class="language-plaintext highlighter-rouge">max_features</code> along with <code class="language-plaintext highlighter-rouge">max_depth</code>, the value around 0.4 seems most promising.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘rf__max_depth’: 8, ‘rf__max_features’: 0.4}</td> <td>0.843307</td> <td>0.053148</td> <td>0.907086</td> <td>0.008095</td> </tr> <tr> <td>{‘rf__max_depth’: 7, ‘rf__max_features’: 0.4}</td> <td>0.830395</td> <td>0.045150</td> <td>0.888562</td> <td>0.012894</td> </tr> <tr> <td>{‘rf__max_depth’: 6, ‘rf__max_features’: ‘sqrt’}</td> <td>0.829707</td> <td>0.041479</td> <td>0.882185</td> <td>0.013430</td> </tr> </tbody> </table> </li> <li> <p>Having <code class="language-plaintext highlighter-rouge">bootstrap = True</code> and <code class="language-plaintext highlighter-rouge">max_samples = 0.7</code> helps in reducing mean_train_score.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘rf__bootstrap’: False, ‘rf__max_depth’: 6, ‘rf__max_features’: 0.3}</td> <td>0.842930</td> <td>0.049107</td> <td>0.881239</td> <td>0.011179</td> </tr> <tr> <td>{‘rf__bootstrap’: True, ‘rf__max_depth’: 5, ‘rf__max_features’: 0.4, ‘rf__max_samples’: 0.7}</td> <td>0.833823</td> <td>0.057091</td> <td>0.867012</td> <td>0.008694</td> </tr> </tbody> </table> </li> <li> <p>Setting <code class="language-plaintext highlighter-rouge">max_leaf_nodes = 100</code> provides more regularization and mean_train_score reduce even further.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘rf__bootstrap’: True, ‘rf__max_depth’: 6, ‘rf__max_features’: 0.5, ‘rf__max_leaf_nodes’: 1000, ‘rf__max_samples’: 0.6}</td> <td>0.838597</td> <td>0.057041</td> <td>0.879224</td> <td>0.012604</td> </tr> <tr> <td>{‘rf__bootstrap’: True, ‘rf__max_depth’: 4, ‘rf__max_features’: 0.3, ‘rf__max_leaf_nodes’: 100, ‘rf__max_samples’: 0.6}</td> <td>0.827642</td> <td>0.053020</td> <td>0.844267</td> <td>0.013082</td> </tr> </tbody> </table> </li> <li> <p>The <code class="language-plaintext highlighter-rouge">criterion</code> of ‘entropy’ or ‘log loss’ gives the best results.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘rf__bootstrap’: True, ‘rf__criterion’: ‘log_loss’, ‘rf__max_depth’: 5, ‘rf__max_features’: 0.5, ‘rf__max_leaf_nodes’: 100, ‘rf__max_samples’: 0.4}</td> <td>0.845795</td> <td>0.046057</td> <td>0.883876</td> <td>0.011191</td> </tr> <tr> <td>{‘rf__bootstrap’: True, ‘rf__criterion’: ‘entropy’, ‘rf__max_depth’: 6, ‘rf__max_features’: 0.4, ‘rf__max_leaf_nodes’: 100, ‘rf__max_samples’: 0.5}</td> <td>0.845477</td> <td>0.036414</td> <td>0.891090</td> <td>0.008494</td> </tr> </tbody> </table> </li> <li> <p>Increasing the number of estimators from the default 100 provides minimal improvement but increases train time drastically, hence default <code class="language-plaintext highlighter-rouge">n_estimators = 100</code> is appropriate.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘rf__bootstrap’: True, ‘rf__criterion’: ‘entropy’, ‘rf__max_depth’: 7, ‘rf__max_features’: 0.5, ‘rf__max_leaf_nodes’: 150, ‘rf__max_samples’: 0.6, ‘rf__n_estimators’: 300}</td> <td>0.858067</td> <td>0.040889</td> <td>0.943415</td> <td>0.004836</td> </tr> <tr> <td>{‘rf__bootstrap’: True, ‘rf__criterion’: ‘entropy’, ‘rf__max_depth’: 6, ‘rf__max_features’: 0.5, ‘rf__max_leaf_nodes’: 50, ‘rf__max_samples’: 0.5, ‘rf__n_estimators’: 100}</td> <td>0.855221</td> <td>0.044152</td> <td>0.914726</td> <td>0.006612</td> </tr> <tr> <td>{‘rf__bootstrap’: True, ‘rf__criterion’: ‘entropy’, ‘rf__max_depth’: 5, ‘rf__max_features’: 0.5, ‘rf__max_leaf_nodes’: 100, ‘rf__max_samples’: 0.4, ‘rf__n_estimators’: 300}</td> <td>0.851057</td> <td>0.046665</td> <td>0.892381</td> <td>0.009649</td> </tr> <tr> <td>{‘rf__bootstrap’: True, ‘rf__criterion’: ‘entropy’, ‘rf__max_depth’: 5, ‘rf__max_features’: 0.5, ‘rf__max_leaf_nodes’: 100, ‘rf__max_samples’: 0.5, ‘rf__n_estimators’: 100}</td> <td>0.849956</td> <td>0.047809</td> <td>0.890280</td> <td>0.009652</td> </tr> </tbody> </table> </li> <li> <p>The original <code class="language-plaintext highlighter-rouge">sampling_strategy</code> provides adequate results.</p> <table> <thead> <tr> <th>Params</th> <th>mean_test_score</th> <th>std_test_score</th> <th>mean_train_score</th> <th>std_test_score</th> </tr> </thead> <tbody> <tr> <td>{‘rf__bootstrap’: True, ‘rf__criterion’: ‘entropy’, ‘rf__max_depth’: 7, ‘rf__max_features’: 0.6, ‘rf__max_leaf_nodes’: 100, ‘rf__max_samples’: 0.5, ‘rf__n_estimators’: 100, ‘rus__sampling_strategy’: 0.02, ‘smote__sampling_strategy’: 0.01}</td> <td>0.861197</td> <td>0.042957</td> <td>0.940184</td> <td>0.006208</td> </tr> <tr> <td>{‘rf__bootstrap’: True, ‘rf__criterion’: ‘entropy’, ‘rf__max_depth’: 5, ‘rf__max_features’: 0.5, ‘rf__max_leaf_nodes’: 100, ‘rf__max_samples’: 0.5, ‘rf__n_estimators’: 100, ‘rus__sampling_strategy’: 0.02, ‘smote__sampling_strategy’: 0.01}</td> <td>0.852273</td> <td>0.045264</td> <td>0.892749</td> <td>0.009282</td> </tr> </tbody> </table> </li> </ol> </details> <p>The optimum hyperparameter combination is as follows :</p> <p><code class="language-plaintext highlighter-rouge">n_estimators = 100</code>|<br> <code class="language-plaintext highlighter-rouge">max_depth = 5</code>|<br> <code class="language-plaintext highlighter-rouge">criterion = entropy</code>|<br> <code class="language-plaintext highlighter-rouge">boostrap = True</code>|<br> <code class="language-plaintext highlighter-rouge">max_samples = 0.5</code>| <br> <code class="language-plaintext highlighter-rouge">max_features = 0.5</code>|<br> <code class="language-plaintext highlighter-rouge">max_leaf_nodes = 100</code>|<br> <code class="language-plaintext highlighter-rouge">smote_sampling_strategy = 0.01</code>|<br> <code class="language-plaintext highlighter-rouge">randomundersampler_sampling_strategy=0.02</code></p> <h2 id="results"> <a class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h2> <p>Finally, the selected models with their tuned hyperparameter combination are evaluated on the test set, which was set aside before the beginning of this exercise.</p> <p>The confusion matrix for Histogram Based Gradient Boosting Classifier on test data (left) and train data (right) is shown below.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/credit_card_fraud_detection/confusion_matrix_hgb-480.webp 480w,/assets/img/credit_card_fraud_detection/confusion_matrix_hgb-800.webp 800w,/assets/img/credit_card_fraud_detection/confusion_matrix_hgb-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/credit_card_fraud_detection/confusion_matrix_hgb.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="confusion matrix for hgb" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/credit_card_fraud_detection/confusion_matrix_hgb_train-480.webp 480w,/assets/img/credit_card_fraud_detection/confusion_matrix_hgb_train-800.webp 800w,/assets/img/credit_card_fraud_detection/confusion_matrix_hgb_train-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/credit_card_fraud_detection/confusion_matrix_hgb_train.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="confusion matrix for hgb on train set" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Confusion matrix for Histogram based Gradient Boosting Classifier </div> <p>The following observations are made :</p> <ul> <li>Out of 98 instances of positive class (fraudulent transactions), 83 are correctly labelled by this model. Thus, 84.69% of fraudulent transactions are correctly identified by the model in unseen data. This is not very far removed from its performance over training data (shown on right), where 85.28% of fraudulent transactions are correctly identified.</li> <li>The identification of fraudulent transactions led to saving 70.46% of the value of transactions involved in fraudulent transactions which amounted to € 8593.</li> <li>The precision on test data is 71.55% while on train data is 72.72%.</li> </ul> <p>Similarly, the confusion matrix for Random Forest Classifier is shown below with test data (left) and train data (right).</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/credit_card_fraud_detection/confusion_matrix_rf-480.webp 480w,/assets/img/credit_card_fraud_detection/confusion_matrix_rf-800.webp 800w,/assets/img/credit_card_fraud_detection/confusion_matrix_rf-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/credit_card_fraud_detection/confusion_matrix_rf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="confusion matrix for rf" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/credit_card_fraud_detection/confusion_matrix_rf_train-480.webp 480w,/assets/img/credit_card_fraud_detection/confusion_matrix_rf_train-800.webp 800w,/assets/img/credit_card_fraud_detection/confusion_matrix_rf_train-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/credit_card_fraud_detection/confusion_matrix_rf_train.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="confusion matrix for rf on train set" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Confusion matrix for Random Forest Classifier </div> <p>The following observations are made :</p> <ul> <li>Out of the total 98 fraudulent transactions, 79 are correctly labelled, thus identifying 80.61% of fraudulent transactions. The performance of this model over training data is higher at 84.52%.</li> <li>The identification of fraudulent transactions led to saving of 67.74% of the value of transactions involved in fraudulent transactions which amounted to € 8261.</li> <li>The precision on test data is 77.45% while on train data is 85.17%</li> </ul> <p>Thus, Histogram Gradient Boosting Classifier provides comparable performance to Random Forest Classifier, but owing to its significantly lower fitting time, it is chosen as the final model.</p> <hr> <h1 id="conclusion"> <a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h1> <p>With this exercise, a program is developed using machine learning which provides good performance in detecting fraudulent transactions. It is able to detect more than 80% of fraudulent transactions, leading to significant savings. If the following metric is extended to value that was lost to fraudulent transactions over the two day period, this can potentially lead to saving \(0.8 \times 60,128 \approx \text{€} 48,100\).</p> <p>At the same time, the precision of the model is above 70%, hence a majority of transactions flagged as fraudulent will actually be fraudulent, thus not causing major inconvenience to genuine transactions.</p> <hr> <h1 id="future-work"> <a class="anchor" href="#future-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>Future work</h1> <p>The current model does not focus on the monetary value of transactions. If weights are attached corresponding to the monetary value involved in a transaction then the model may perform better. This could be a possible area of work in the future.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Vipul Yadav. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: March 08, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e9d2875689634365f374b5a946cee815"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="https://cloud.umami.is/script.js" data-website-id="63e2f685-288d-42d7-83ca-f8e649875af7"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>